{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### WIMP (What's in My Pot?) - Oxford Nanopore's metagenomic classifier.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all the modules that we will need\n",
    "import os, h5py, subprocess\n",
    "import numpy as np\n",
    "import pandas\n",
    "%load_ext rpy2.ipython\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from ete3 import NCBITaxa\n",
    "ncbi = NCBITaxa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we need to set our directories:\n",
    "HOME_DIR = os.environ.get('PORECAMPAU_ANALYSIS_PATH') + \"/\"\n",
    "PORECAMPAU_DATA_DIR = os.environ.get('PORECAMPAU_DATA_PATH') + \"/\"\n",
    "\n",
    "LAKE_HILLIER_FAST5_DIR = PORECAMPAU_DATA_DIR + \"fast5/\"\n",
    "METAGENOMICS_DIR = PORECAMPAU_DATA_DIR + \"metagenomics/\"\n",
    "\n",
    "LOCAL_LAKE_HILLIER_DIR = HOME_DIR + \"lake_hillier/\"\n",
    "    \n",
    "# Create the local LAKE HILLIER DIRECTORY\n",
    "if not os.path.isdir(LOCAL_LAKE_HILLIER_DIR):\n",
    "    os.mkdir(LOCAL_LAKE_HILLIER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This dataset has been returned from the metrichor cloud.\n",
    "Like the fastq mux, and hairpin attribute of normal 2D reads, it also contains a classification summary attribute.\n",
    "This holds the taxonomic ID that was assigned to this read.\n",
    "\n",
    "Let's extract that information from each fast5 file in the list and write this into a tsv file.\n",
    "(A text file with two tabs, one for the read name another for the taxonomic id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fast5_files = [LAKE_HILLIER_FAST5_DIR + fast5_file for fast5_file in os.listdir(LAKE_HILLIER_FAST5_DIR)\n",
    "                if fast5_file.endswith(\".fast5\")]\n",
    "hdf5_workflow_path = \"Analyses/Classification_000/Summary\"\n",
    "hdf5_taxid_path = hdf5_workflow_path + \"/classification_2d\"\n",
    "tsv_file = LOCAL_LAKE_HILLIER_DIR + \"my_taxid_tsv.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the tsv handler\n",
    "tsv_handler = open(tsv_file, 'w')\n",
    "for fast5_file in fast5_files:\n",
    "    # Read in the fast5 file into a handler variable\n",
    "    f = h5py.File(fast5_file, 'r')\n",
    "    # The try command is used so that we can catch any errors.\n",
    "    # This prevents the script from stalling if we encounter a read that\n",
    "    # does not have the classification strand attribute.\n",
    "    try:  \n",
    "        f = h5py.File(fast5_file, 'r')  # r is for read.\n",
    "        workflow_status = f[hdf5_workflow_path].attrs.values()[0] \n",
    "        if not workflow_status == \"Workflow successful\":\n",
    "            continue  # No sequence classified, let's move onto the next sequence.\n",
    "        # We don't need to use the else statement since we've used the 'continue'\n",
    "        # statement in the previous line.\n",
    "        taxid = f[hdf5_taxid_path].attrs.values()[0]\n",
    "        # Write file name and taxid to our tsv file.\n",
    "        tsv_handler.write(\"%s \\t %s \\n\" % (fast5_file, taxid))  # The \\t means tab while the \\n means new line.\n",
    "    except KeyError:\n",
    "        print(\"This file does not have the classification attribute\")\n",
    "\n",
    "# Close the file. Programmers who don't close files are as bad as housemates who don't\n",
    "# shut the front door of your house.\n",
    "tsv_handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Great work, now open up this tsv file using the file management system so we can get a grasp of what is happening.\n",
    "\n",
    "There were 100 fast5 files in the folder but only 70 lines here. It seems that some of these reads weren't successfully aligned to the WIMP database.\n",
    "\n",
    "Now let's grab the tsv from the entire set of pass reads. We might be able to make use of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's get the full array and load that into python.\n",
    "# We can then parse that array into R to return a summary of the list.\n",
    "full_tsv = METAGENOMICS_DIR\n",
    "full_tsv += \"lake_hillier_taxids_by_WIMP.tsv\"\n",
    "\n",
    "taxids = np.loadtxt(full_tsv, delimiter=\"\\t\", usecols=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R -i taxids -o taxids_summary\n",
    "# Generates a table of the the number of taxids and how many reads matched to that id.\n",
    "taxids_summary <- as.data.frame(table(taxids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can use the ete3 package to then generate a lineage tree for each taxid.\n",
    "\n",
    "# Write the summarised version to file.\n",
    "lineage_file = LOCAL_LAKE_HILLIER_DIR + \"lake_hillier_lineages.tsv\"\n",
    "lineage_file_h = open(lineage_file, 'w')\n",
    "\n",
    "for index, row in taxids_summary.iterrows():\n",
    "    taxid = row['taxids']\n",
    "    freq = str(row['Freq'])  # Freq is the column name automatically generated by R table\n",
    "    # Generate the lineage for the given taxid\n",
    "    # Returns a list of taxids.\n",
    "    try:\n",
    "        lineage = ncbi.get_lineage(taxid)\n",
    "    except ValueError:  # Can occur with different NCBI database versions\n",
    "            print(\"Warning: %s not found in database\" % taxid)  \n",
    "                                                             \n",
    "    names = ncbi.get_taxid_translator(lineage)\n",
    "    lineage_names = [names[taxid] for taxid in lineage]\n",
    "    # We have to add the 1: to not write the 'root' kingdom in the lineage.\n",
    "    lineage_file_h.write(freq + \"\\t\" + \"\\t\".join(lineage_names[1:]) + \"\\n\")\n",
    "\n",
    "lineage_file_h.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the lineage_file before moving on to get an appreciation of the input for a Krona file. Run the following command to then generate a Krona plot of Lake Hillier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "krona_output_file = LOCAL_LAKE_HILLIER_DIR + \"krona_output_lake_hillier.html\"\n",
    "# We want the name variable to go onto the command line with the double quotes,\n",
    "# so we mask it with single quotes first.\n",
    "krona_output_name = '\"Lake Hillier - Sediment, MinION R7.3 Q2 2015\"'\n",
    "\n",
    "\n",
    "ktImportCommand = \"ktImportText -o %s -n %s %s\" % (krona_output_file, krona_output_name, lineage_file)\n",
    "stderr = subprocess.check_output(ktImportCommand, shell=True, stderr=subprocess.STDOUT)\n",
    "print(\"Stderr = %s\" % stderr)\n",
    "\n",
    "krona_output_file_quotes = '\"' + krona_output_file.replace(HOME_DIR, \"../\") + '\"'\n",
    "print \"Add the following to the src variable in the html plot\"\n",
    "print krona_output_file_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%HTML -i krona_output_file_quotes\n",
    "<iframe width=\"200%\" height=\"700\" src=\"../lake_hillier/krona_output_lake_hillier.html\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}